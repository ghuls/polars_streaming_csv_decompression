from __future__ import annotations

import io
from typing import IO, TYPE_CHECKING, Callable

import polars as pl
import xopen
from polars.datatypes.constants import N_INFER_DEFAULT

# Used to register a new generator on every instantiation.
from polars.io.plugins import register_io_source

if TYPE_CHECKING:
    from collections.abc import Iterator, Sequence
    from pathlib import Path

    from polars._typing import CsvEncoding, PolarsDataType, SchemaDict


def streaming_csv(
    source: str
    | Path
    | IO[str]
    | IO[bytes]
    | bytes
    | list[str]
    | list[Path]
    | list[IO[str]]
    | list[IO[bytes]]
    | list[bytes],
    *,
    has_header: bool = True,
    separator: str = ",",
    comment_prefix: str | None = None,
    quote_char: str | None = '"',
    skip_rows: int = 0,
    skip_lines: int = 0,
    schema: SchemaDict | None = None,
    schema_overrides: SchemaDict | Sequence[PolarsDataType] | None = None,
    null_values: str | Sequence[str] | dict[str, str] | None = None,
    missing_utf8_is_empty_string: bool = False,
    ignore_errors: bool = False,
    cache: bool = True,
    with_column_names: Callable[[list[str]], list[str]] | None = None,
    infer_schema: bool = True,
    infer_schema_length: int | None = N_INFER_DEFAULT,
    n_rows: int | None = None,
    encoding: CsvEncoding = "utf8",
    low_memory: bool = False,
    rechunk: bool = False,
    skip_rows_after_header: int = 0,
    row_index_name: str | None = None,
    row_index_offset: int = 0,
    try_parse_dates: bool = False,
    eol_char: str = "\n",
    new_columns: Sequence[str] | None = None,
    raise_if_empty: bool = True,
    truncate_ragged_lines: bool = False,
    decimal_comma: bool = False,
    # Following `pl.scan_csv` parameters are not supported (yet)
    # glob: bool = True,
    # storage_options: dict[str, Any] | None = None,
    # credential_provider: CredentialProviderFunction | Literal["auto"] | None = "auto",
    # retries: int = 2,
    # file_cache_ttl: int | None = None,
    # include_file_paths: str | None = None,
) -> pl.LazyFrame:
    r"""
    Lazily read from a compressed CSV file by streaming the decompression step.

    Lazy reading allows the query optimizer to push down predicates
    and projections to the scan level, thereby potentially reducing
    memory overhead.

    This function is similar to `pl.scan_csv`, but it does decompress
    the compressed CSV file in chuncks instead of decompressing the
    whole file in memory first.

    Globbing and Cloud related options of `pl.scan_csv` are not supported.

    Decompression is handled by `xopen`, which supports the following
    compression formats: `gzip`, `bzip2`, `xz` and `Zstandard`.

    Parameters
    ----------
    source
        Path to a compressed CSV file.
    has_header
        Indicate if the first row of the dataset is a header or not. If set to False,
        column names will be autogenerated in the following format: `column_x`, with
        `x` being an enumeration over every column in the dataset, starting at 1.
    separator
        Single byte character to use as separator in the file.
    comment_prefix
        A string used to indicate the start of a comment line. Comment lines are skipped
        during parsing. Common examples of comment prefixes are `#` and `//`.
    quote_char
        Single byte character used for csv quoting, default = `"`.
        Set to None to turn off special handling and escaping of quotes.
    skip_rows
        Start reading after ``skip_rows`` rows. The header will be parsed at this
        offset. Note that we respect CSV escaping/comments when skipping rows.
        If you want to skip by newline char only, use `skip_lines`.
    skip_lines
        Start reading after `skip_lines` lines. The header will be parsed at this
        offset. Note that CSV escaping will not be respected when skipping lines.
        If you want to skip valid CSV rows, use ``skip_rows``.
    schema
        Provide the schema. This means that polars doesn't do schema inference.
        This argument expects the complete schema, whereas `schema_overrides` can be
        used to partially overwrite a schema.
    schema_overrides
        Overwrite dtypes during inference; should be a {colname:dtype,} dict or,
        if providing a list of strings to `new_columns`, a list of dtypes of
        the same length.
    null_values
        Values to interpret as null values. You can provide a:

        - `str`: All values equal to this string will be null.
        - `List[str]`: All values equal to any string in this list will be null.
        - `Dict[str, str]`: A dictionary that maps column name to a
          null value string.

    missing_utf8_is_empty_string
        By default a missing value is considered to be null; if you would prefer missing
        utf8 values to be treated as the empty string you can set this param True.
    ignore_errors
        Try to keep reading lines if some lines yield errors.
        First try `infer_schema=False` to read all columns as
        `pl.String` to check which values might cause an issue.
    cache
        Cache the result after reading.
    with_column_names
        Apply a function over the column names just in time (when they are determined);
        this function will receive (and should return) a list of column names.
    infer_schema
        When `True`, the schema is inferred from the data using the first
        `infer_schema_length` rows.
        When `False`, the schema is not inferred and will be `pl.String` if not
        specified in `schema` or `schema_overrides`.
    infer_schema_length
        The maximum number of rows to scan for schema inference.
        If set to `None`, the full data may be scanned *(this is slow)*.
        Set `infer_schema=False` to read all columns as `pl.String`.
    n_rows
        Stop reading from CSV file after reading `n_rows`.
    encoding : {'utf8', 'utf8-lossy', 'windows-1252', 'windows-1252-lossy', ...}
        Lossy means that invalid utf8 values are replaced with `ï¿½`
        characters. Defaults to "utf8".
    low_memory
        Reduce memory pressure at the expense of performance.
    rechunk
        Reallocate to contiguous memory when all chunks/ files are parsed.
    skip_rows_after_header
        Skip this number of rows when the header is parsed.
    row_index_name
        If not None, this will insert a row index column with the given name into
        the DataFrame.
    row_index_offset
        Offset to start the row index column (only used if the name is set).
    try_parse_dates
        Try to automatically parse dates. Most ISO8601-like formats
        can be inferred, as well as a handful of others. If this does not succeed,
        the column remains of data type `pl.String`.
    eol_char
        Single byte end of line character (default: `\n`). When encountering a file
        with windows line endings (`\r\n`), one can go with the default `\n`. The extra
        `\r` will be removed when processed.
    new_columns
        Provide an explicit list of string column names to use (for example, when
        scanning a headerless CSV file). If the given list is shorter than the width of
        the DataFrame the remaining columns will have their original name.
    raise_if_empty
        When there is no data in the source, `NoDataError` is raised. If this parameter
        is set to False, an empty LazyFrame (with no columns) is returned instead.
    truncate_ragged_lines
        Truncate lines that are longer than the schema.
    decimal_comma
        Parse floats using a comma as the decimal separator instead of a period.

    Returns
    -------
    LazyFrame

    See Also
    --------
    scab_csv : Lazily read from a CSV file or multiple files via glob patterns.

    Examples
    --------
    >>> import pathlib
    >>> import polars as pl
    >>> import polars_streaming_csv_decompression
    >>>
    >>> (
    ...     polars_streaming_csv_decompression.streaming_csv(
    ...         "my_big_file.csv.gz"
    ...     )  # lazy, doesn't do a thing
    ...     .select(
    ...         ["a", "c"]
    ...     )  # select only 2 columns (other columns will not be read)
    ...     .filter(
    ...         pl.col("a") > 10
    ...     )  # the filter is pushed down the scan, so less data is read into memory
    ...     .head(100)  # constrain number of returned results to 100
    ... )  # doctest: +SKIP

    """

    def get_schema() -> SchemaDict:
        if schema:
            # Schema is provided, return it.
            return schema

        eol_char_as_bytes = eol_char.encode("utf-8")

        # Write decompressed data to a BytesIO object.
        csv_bytesio = io.BytesIO()

        has_utf8_utf8_lossy_encoding = (
            encoding in {"utf8", "utf8-lossy"} if encoding else True
        )
        encoding_str = encoding if encoding else "utf8"
        encoding_str, encoding_errors = (
            (encoding_str[:-6], "replace")
            if encoding_str.endswith("-lossy")
            else (encoding_str, "strict")
        )

        with xopen.xopen(source, "rb") as fh_compressed_csv:
            if infer_schema_length is None:
                if not has_utf8_utf8_lossy_encoding:
                    with io.TextIOWrapper(
                        fh_compressed_csv, encoding=encoding_str, errors=encoding_errors
                    ) as csv_textio:
                        # Decode CSV file in block with provided encoding which
                        # can be optionally lossy and encode output as `utf8`.
                        block = csv_textio.read(1024 * 128 * 10).encode("utf8")

                        while len(block) > 0:
                            csv_bytesio.write(block)
                            block = csv_textio.read(1024 * 128 * 10).encode("utf8")

                        csv_bytesio.seek(0)

                # Read the whole file to infer schema.
                lf = pl.scan_csv(
                    fh_compressed_csv.read()
                    if has_utf8_utf8_lossy_encoding
                    else csv_bytesio.read(),
                    has_header=has_header,
                    separator=separator,
                    comment_prefix=comment_prefix,
                    quote_char=quote_char,
                    skip_rows=skip_rows,
                    skip_lines=skip_lines,
                    schema=schema,
                    schema_overrides=schema_overrides,
                    null_values=null_values,
                    missing_utf8_is_empty_string=missing_utf8_is_empty_string,
                    ignore_errors=ignore_errors,
                    cache=cache,
                    with_column_names=with_column_names,
                    infer_schema=infer_schema,
                    infer_schema_length=infer_schema_length,
                    n_rows=n_rows,
                    encoding=encoding if has_utf8_utf8_lossy_encoding else "utf8",
                    low_memory=low_memory,
                    rechunk=rechunk,
                    skip_rows_after_header=skip_rows_after_header,
                    row_index_name=row_index_name,
                    row_index_offset=row_index_offset,
                    try_parse_dates=try_parse_dates,
                    eol_char=eol_char,
                    new_columns=new_columns,
                    raise_if_empty=raise_if_empty,
                    truncate_ragged_lines=truncate_ragged_lines,
                    decimal_comma=decimal_comma,
                    # Globbing is not supported yet.
                    glob=False,
                )

                return lf.collect_schema()

            # When `infer_schema=False` all columns are read as `pl.String`, so only
            # read N_INFER_DEFAULT rows.
            infer_schema_length_int = (
                N_INFER_DEFAULT if infer_schema_length is False else infer_schema_length
            )

            # Keep track of the number of newline characters seen.
            n_seen_new_lines = 0

            # Keep track of the trailing data of a block (data after the last newline
            # character).
            block_trailing_data = b""

            while n_seen_new_lines < infer_schema_length_int:
                block = fh_compressed_csv.read(1024 * 128 * 10)

                if block:
                    # Find last newline character in block.
                    last_newline_in_block = block.rfind(eol_char_as_bytes)

                    if last_newline_in_block == -1:
                        msg = "No EOL character found in current block"
                        raise ValueError(msg)

                    # Count newline characters in block.
                    n_seen_new_lines += block[:last_newline_in_block].count(
                        eol_char_as_bytes
                    )

                    # Write trailing data from previous block to BytesIO object.
                    csv_bytesio.write(block_trailing_data)

                    # Write block (till last new line) to BytesIO object.
                    csv_bytesio.write(block[:last_newline_in_block])

                    # Save trailing data after last newline character of the current
                    # block for next iteration.
                    block_trailing_data = block[last_newline_in_block:]
                else:
                    # End of file.
                    break

            # Seek to beginning of BytesIO object, so `pl.scan_csv` can read it from
            # the start.
            csv_bytesio.seek(0)

            if not has_utf8_utf8_lossy_encoding:
                # Decode current data in `csv_bytesio` with provided encoding which
                # can be optionally lossy and encode output as `utf8`.
                csv_bytesio.write(
                    csv_bytesio.getvalue()
                    .decode(encoding_str, errors=encoding_errors)
                    .encode("utf8")
                )
                csv_bytesio.truncate()
                csv_bytesio.seek(0)

            lf = pl.scan_csv(
                csv_bytesio,
                has_header=has_header,
                separator=separator,
                comment_prefix=comment_prefix,
                quote_char=quote_char,
                skip_rows=skip_rows,
                skip_lines=skip_lines,
                schema=None,
                schema_overrides=schema_overrides,
                null_values=null_values,
                missing_utf8_is_empty_string=missing_utf8_is_empty_string,
                ignore_errors=ignore_errors,
                cache=cache,
                with_column_names=with_column_names,
                infer_schema=infer_schema,
                infer_schema_length=infer_schema_length,
                n_rows=n_rows,
                encoding=encoding_str if has_utf8_utf8_lossy_encoding else "utf8",
                low_memory=low_memory,
                rechunk=rechunk,
                skip_rows_after_header=skip_rows_after_header,
                row_index_name=row_index_name,
                row_index_offset=row_index_offset,
                try_parse_dates=try_parse_dates,
                eol_char=eol_char,
                new_columns=new_columns,
                raise_if_empty=raise_if_empty,
                truncate_ragged_lines=truncate_ragged_lines,
                decimal_comma=decimal_comma,
                # Globbing is not supported yet.
                glob=False,
            )

            return lf.collect_schema()

    def streaming_csv_decompression_generator(
        with_columns: list[str] | None,
        predicate: pl.Expr | None,
        n_rows: int | None,
        batch_size: int | None,
    ) -> Iterator[pl.DataFrame]:
        comment_prefix_as_bytes = (
            comment_prefix.encode("utf-8") if comment_prefix else None
        )
        eol_char_as_bytes = eol_char[0].encode("utf-8")

        n_seen_rows = 0
        n_rows_still_to_read = n_rows
        n_rows_before_header_skipped = 0
        n_rows_header_and_after_header_skipped = 0

        next_newline_in_block = -1
        row_index_offset_adjusted = row_index_offset
        csv_bytesio = io.BytesIO()

        has_utf8_utf8_lossy_encoding = (
            encoding in {"utf8", "utf8-lossy"} if encoding else True
        )
        encoding_str = encoding if encoding else "utf8"
        encoding_str, encoding_errors = (
            (encoding_str[:-6], "replace")
            if encoding_str.endswith("-lossy")
            else (encoding_str, "strict")
        )

        with xopen.xopen(source, "rb") as fh:
            # Decompress CSV/TSV file in blocks and yield DataFrames for each block.
            while n_rows_still_to_read is None or n_rows_still_to_read > 0:
                block = fh.read(1024 * 128 * 10)
                if block:
                    if n_seen_rows == 0:
                        # Skip certain rows at start of file before passing data to
                        # `pl.scan_csv`:
                        #   - Skip rows before header: `skip_rows` + `skip_lines` > 0
                        #   - Skip comment lines before header line for lines that
                        #     start with `comment_prefix`.
                        #   - Skip header line if `has_header=True`.
                        #   - Skip rows after header: `skip_rows_after_header` > 0

                        skip_rows_before_header = skip_rows + skip_lines

                        # Skip rows before header.
                        if n_rows_before_header_skipped < skip_rows_before_header:
                            while (
                                n_rows_before_header_skipped < skip_rows_before_header
                            ):
                                next_newline_in_block = block.find(
                                    eol_char_as_bytes, next_newline_in_block + 1
                                )

                                if next_newline_in_block == -1:
                                    # No more newline characters in current block.
                                    # Will trigger reading next block below.
                                    break

                                # Increment number of rows skipped before header if
                                # newline was found.
                                n_rows_before_header_skipped += 1

                            if n_rows_before_header_skipped < skip_rows_before_header:
                                # Read next block to find the next newline character.
                                continue

                            # Update block so it does not contain the skipped rows
                            # before the header.
                            block = block[next_newline_in_block + 1 :]
                            next_newline_in_block = -1

                        # Skip comment lines before the header if comment prefix is set.
                        if comment_prefix_as_bytes:
                            # Skip lines starting with comment prefix.
                            while block.startswith(comment_prefix_as_bytes):
                                next_newline_in_block = block.find(eol_char_as_bytes)
                                if next_newline_in_block == -1:
                                    # No more newline characters in current block.
                                    # Will trigger reading next block below.
                                    break

                                # Update block so it does not contain the comment line.
                                block = block[next_newline_in_block + 1 :]
                                next_newline_in_block = -1

                            if block.startswith(comment_prefix_as_bytes):
                                # Read next block to find the next newline character for
                                # the current comment line.
                                # As the next read block will not contain the comment
                                # prefix anymore at the start, skip the line by
                                # decrementing `n_rows_before_header_skipped`, so the
                                # skip rows before header logic will remove that partial
                                # comment line of the next block.
                                n_rows_before_header_skipped -= 1
                                continue

                        # Remove header line if `has_header=True` as the schema is
                        # already set and `pl.scan_csv` should not see that line.
                        skip_header_and_rows_after_header = (
                            int(has_header) + skip_rows_after_header
                        )

                        # Skip header and rows after header.
                        if skip_header_and_rows_after_header > 0:
                            while (
                                n_rows_header_and_after_header_skipped
                                < skip_header_and_rows_after_header
                            ):
                                next_newline_in_block = block.find(
                                    eol_char_as_bytes, next_newline_in_block + 1
                                )

                                if next_newline_in_block == -1:
                                    break

                                n_rows_header_and_after_header_skipped += 1

                            if (
                                n_rows_header_and_after_header_skipped
                                < skip_header_and_rows_after_header
                            ):
                                # Read next block to find the next newline character.
                                continue

                            # Update block so it does not contain the skipped header and
                            # rows after header.
                            block = block[next_newline_in_block + 1 :]

                    # Find last newline character in block, so `pl.scan_csv` will
                    # always get a complete record as last record.
                    last_newline_in_block = block.rfind(eol_char_as_bytes)

                    if last_newline_in_block == -1:
                        msg = "No EOL character found in current block"
                        raise ValueError(msg)

                    # Write block till last newline to BytesIO object, truncate BytesIO
                    # object to current write position (BytesIO object could be larger
                    # from previous iteration) and seek to the beginning so
                    # `pl.scan_csv` can read from the start.
                    csv_bytesio.write(block[:last_newline_in_block])
                    csv_bytesio.truncate()
                    csv_bytesio.seek(0)

                    if not has_utf8_utf8_lossy_encoding:
                        # Decode current data in `csv_bytesio` with provided encoding
                        # which can be optionally lossy and encode output as `utf8`.
                        csv_bytesio.write(
                            csv_bytesio.getvalue()
                            .decode(encoding_str, errors=encoding_errors)
                            .encode("utf8")
                        )
                        csv_bytesio.truncate()
                        csv_bytesio.seek(0)

                    schema_of_csv_for_scan_csv = schema_of_csv.copy()

                    if row_index_name is not None:
                        # Remove row index name from schema for `pl.scan_csv` as it
                        # will be added by `pl.scan_csv` itself.
                        schema_of_csv_for_scan_csv.pop(row_index_name)

                    # Scan CSV/TSV file from BytesIO object:
                    #   - As schema is already inferred or provided by the user,
                    #     all other schema related options are disabled here.
                    #   - Header lines and other rows are already skipped above and
                    #     are not included in the BytesIO object.
                    lf = pl.scan_csv(
                        csv_bytesio,
                        has_header=False,
                        separator=separator,
                        comment_prefix=comment_prefix,
                        quote_char=quote_char,
                        skip_rows=0,
                        skip_lines=0,
                        schema=schema_of_csv_for_scan_csv,
                        schema_overrides=None,
                        null_values=null_values,
                        missing_utf8_is_empty_string=missing_utf8_is_empty_string,
                        ignore_errors=ignore_errors,
                        cache=cache,
                        with_column_names=with_column_names,
                        infer_schema=False,
                        infer_schema_length=infer_schema_length,
                        n_rows=n_rows_still_to_read,
                        encoding=encoding_str
                        if has_utf8_utf8_lossy_encoding
                        else "utf8",
                        low_memory=low_memory,
                        rechunk=rechunk,
                        skip_rows_after_header=0,
                        row_index_name=row_index_name,
                        row_index_offset=row_index_offset_adjusted,
                        try_parse_dates=try_parse_dates,
                        eol_char=eol_char,
                        new_columns=new_columns,
                        raise_if_empty=raise_if_empty,
                        truncate_ragged_lines=truncate_ragged_lines,
                        decimal_comma=decimal_comma,
                        # Globbing is not supported yet.
                        glob=False,
                    )

                    # Only keep columns that are needed.
                    if with_columns is not None:
                        lf = lf.select(with_columns)

                    # Filter out rows that don't match the predicate.
                    if predicate is not None:
                        lf = lf.filter(predicate)

                    # Limit the number of rows to read.
                    if n_rows_still_to_read is not None:
                        lf = lf.head(n_rows_still_to_read)

                    # TODO: Implement batch_size limit.

                    df = lf.collect()

                    n_seen_rows += df.height
                    if n_rows_still_to_read is not None:
                        n_rows_still_to_read -= df.height
                    row_index_offset_adjusted = (
                        row_index_offset + n_seen_rows if row_index_offset else None
                    )

                    yield df

                    # Seek to start of BytesIO object and write data of the current
                    # block located after the last newline character for the next
                    # iteration.
                    csv_bytesio.seek(0)
                    csv_bytesio.write(block[last_newline_in_block:])
                else:
                    break

    # Get schema from compressed CSV/TSV file.
    schema_of_csv = get_schema()

    return register_io_source(
        io_source=streaming_csv_decompression_generator, schema=schema_of_csv
    )
